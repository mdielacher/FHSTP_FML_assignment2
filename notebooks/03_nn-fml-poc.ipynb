{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, confusion_matrix\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import flwr as fl\n",
    "from flwr.common import Metrics\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for reproducibility\n",
    "seed_value = 42\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "tf.keras.utils.set_random_seed(seed_value)\n",
    "VERBOSE = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_a = pd.read_csv('../data/BankA.csv')\n",
    "df_bank_b = pd.read_csv('../data/BankB.csv')\n",
    "df_bank_c = pd.read_csv('../data/BankC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_bank_a, df_bank_b, df_bank_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip all string values from the dataset\n",
    "df = df_all.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Never-worked and Without-pay into one category\n",
    "df['workclass'] = df['workclass'].replace(['Never-worked', 'Without-pay'], 'Not-working')\n",
    "df['workclass'] = df['workclass'].replace(['?', '*'], 'unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Married-civ-spouse and Married-AF-spouse into one category\n",
    "df['marital-status'] = df['marital-status'].replace(['Married-civ-spouse', 'Married-AF-spouse'], 'Married')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace occupation by 4 categories (low, medium, high, unknown)\n",
    "df['occupation'] = df['occupation'].replace(['Exec-managerial', 'Prof-specialty'], 'high')\n",
    "df['occupation'] = df['occupation'].replace(['Armed-Forces', 'Protective-serv', 'Tech-support', 'Sales', 'Craft-repair', 'Transport-moving'], 'medium')\n",
    "df['occupation'] = df['occupation'].replace(['Adm-clerical', 'Machine-op-inspct', 'Farming-fishing', 'Handlers-cleaners', 'Other-service', 'Priv-house-serv'], 'low')\n",
    "df['occupation'] = df['occupation'].replace(['?', '*'], 'unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Husband and Wife into one category\n",
    "df['relationship'] = df['relationship'].replace(['Husband', 'Wife'], 'Parent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map native-country to continents\n",
    "df['native-country'] = df['native-country'].str.strip()\n",
    "df['native-country'] = df['native-country'].replace(['United-States', 'Puerto-Rico', 'Canada', 'Outlying-US(Guam-USVI-etc)', 'Cuba', 'Jamaica', 'Mexico', 'Dominican-Republic', 'El-Salvador', 'Guatemala', 'Haiti', 'Honduras', 'Nicaragua', 'Trinadad&Tobago', 'Peru', 'Ecuador', 'Columbia', 'Honduras', 'Haiti', 'Guatemala', 'El-Salvador', 'Dominican-Republic', 'Columbia', 'Ecuador', 'Peru', 'Jamaica', 'Mexico', 'Puerto-Rico', 'Cuba', 'Outlying-US(Guam-USVI-etc)', 'Canada', 'United-States'], 'North-America')\n",
    "df['native-country'] = df['native-country'].replace(['Germany', 'England', 'Italy', 'Poland', 'Portugal', 'Ireland', 'France', 'Yugoslavia', 'Scotland', 'Greece', 'Hungary', 'Holand-Netherlands'], 'Europe')\n",
    "df['native-country'] = df['native-country'].replace(['Philippines', 'India', 'China', 'Japan', 'Vietnam', 'Taiwan', 'Iran', 'Thailand', 'Hong', 'Cambodia', 'Laos'], 'Asia')\n",
    "df['native-country'] = df['native-country'].replace(['South', 'Columbia', 'Ecuador', 'Peru'], 'South-America')\n",
    "df['native-country'] = df['native-country'].replace(['Trinadad&Tobago', 'Honduras', 'Haiti', 'Guatemala', 'El-Salvador', 'Dominican-Republic', 'Columbia', 'Ecuador', 'Peru'], 'Central-America')\n",
    "df['native-country'] = df['native-country'].replace(['?', '*'], 'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['education'] = df['education'].replace(['Preschool', '1st-4th', '5th-6th', '7th-8th', '9th', '10th', '11th', '12th'], 'school')\n",
    "df['education'] = df['education'].replace(['Assoc-voc', 'Assoc-acdm', 'Prof-school', 'Some-college'], 'higher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caclulate capital-diff\n",
    "df['capital-diff'] = df['capital-gain'] - df['capital-loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['race'] = df['race'].replace(['Black', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'Other'], 'Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert age in bins of 10 years\n",
    "df['age'] = pd.cut(df['age'], bins=[0, 20, 30, 40, 50, 60, 70, 80, 90, 100], labels=['0-20', '20-30', '30-40', '40-50', '50-60', '60-70', '70-80', '80-90', '>90'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the fnlwgt column\n",
    "df.drop(['fnlwgt'], axis=1, inplace=True)\n",
    "\n",
    "# Drop education-num column\n",
    "df.drop(['educational-num'], axis=1, inplace=True)\n",
    "\n",
    "# Drop the gender column\n",
    "df.drop(['gender'], axis=1, inplace=True)\n",
    "\n",
    "# Drop the race column\n",
    "df.drop('race', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace income by 0 and 1\n",
    "df['income'] = df['income'].map({'<=50K': 0, '>50K': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: institute\n",
      "institute\n",
      "Bank B    403240\n",
      "Bank A    226164\n",
      "Bank C    170595\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n",
      "Column: workclass\n",
      "workclass\n",
      "Private             546342\n",
      "Self-emp-not-inc     66145\n",
      "Local-gov            51137\n",
      "unknown              47431\n",
      "State-gov            34717\n",
      "Self-emp-inc         27715\n",
      "Federal-gov          25879\n",
      "Not-working            633\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n",
      "Column: education\n",
      "education\n",
      "HS-grad      258661\n",
      "higher       241834\n",
      "Bachelors    133796\n",
      "school       110209\n",
      "Masters       45697\n",
      "Doctorate      9802\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n",
      "Column: marital-status\n",
      "marital-status\n",
      "Married                  368820\n",
      "Never-married            250510\n",
      "Divorced                 110459\n",
      "Widowed                   34203\n",
      "Separated                 25566\n",
      "Married-spouse-absent     10441\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n",
      "Column: occupation\n",
      "occupation\n",
      "low        281223\n",
      "medium     259651\n",
      "high       211330\n",
      "unknown     47795\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n",
      "Column: relationship\n",
      "relationship\n",
      "Parent            362767\n",
      "Not-in-family     212898\n",
      "Own-child         119123\n",
      "Unmarried          80532\n",
      "Other-relative     24679\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n",
      "Column: native-country\n",
      "native-country\n",
      "North-America    753935\n",
      "Asia              14694\n",
      "Unknown           14390\n",
      "Europe            14273\n",
      "South-America      2707\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n",
      "Column: capital-gain\n",
      "(-100.0, 16666.5]     792260\n",
      "(83332.5, 99999.0]      4665\n",
      "(16666.5, 33333.0]      3019\n",
      "(33333.0, 49999.5]        55\n",
      "(49999.5, 66666.0]         0\n",
      "(66666.0, 83332.5]         0\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n",
      "Column: capital-loss\n",
      "(-3.6839999999999997, 613.833]    760671\n",
      "(1841.5, 2455.333]                 25034\n",
      "(1227.667, 1841.5]                 12776\n",
      "(2455.333, 3069.167]                 788\n",
      "(613.833, 1227.667]                  540\n",
      "(3069.167, 3683.0]                   190\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n",
      "Column: hours-per-week\n",
      "(33.667, 50.0]      555199\n",
      "(17.333, 33.667]    105827\n",
      "(50.0, 66.333]       72075\n",
      "(0.901, 17.333]      44806\n",
      "(66.333, 82.667]     16324\n",
      "(82.667, 99.0]        5768\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n",
      "Column: income\n",
      "(-0.002, 0.167]    601449\n",
      "(0.833, 1.0]       198550\n",
      "(0.167, 0.333]          0\n",
      "(0.333, 0.5]            0\n",
      "(0.5, 0.667]            0\n",
      "(0.667, 0.833]          0\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n",
      "Column: capital-diff\n",
      "(-3786.683, 13597.333]    783776\n",
      "(13597.333, 30877.667]     11477\n",
      "(82718.667, 99999.0]        4665\n",
      "(30877.667, 48158.0]          81\n",
      "(48158.0, 65438.333]           0\n",
      "(65438.333, 82718.667]         0\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    " \n",
    "for column in df[categorical_columns].columns:\n",
    "    print(f\"Column: {column}\")\n",
    "    print(df[column].value_counts())\n",
    "    print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# Print also the numberical columns, categorize them into bins of 6\n",
    "numerical_columns = df.select_dtypes(include=['int64']).columns\n",
    "\n",
    "for column in df[numerical_columns].columns:\n",
    "    print(f\"Column: {column}\")\n",
    "    print(df[column].value_counts(bins=6))\n",
    "    print(\"\\n\" + \"=\"*30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the categorical columns\n",
    "categorical_columns = categorical_columns.append(pd.Index(['age']))\n",
    "df = pd.get_dummies(df, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop institutes columns\n",
    "df_all = df.drop(['institute_Bank A', 'institute_Bank B', 'institute_Bank C'], axis=1)\n",
    "df_bank_a = df[df['institute_Bank A'] == 1].drop(['institute_Bank A', 'institute_Bank B', 'institute_Bank C'], axis=1)\n",
    "df_bank_b = df[df['institute_Bank B'] == 1].drop(['institute_Bank A', 'institute_Bank B', 'institute_Bank C'], axis=1)\n",
    "df_bank_c = df[df['institute_Bank C'] == 1].drop(['institute_Bank A', 'institute_Bank B', 'institute_Bank C'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in df_all:    799999\n",
      "Number of rows in df_bank_a: 226164\n",
      "Number of rows in df_bank_b: 403240\n",
      "Number of rows in df_bank_c: 170595\n"
     ]
    }
   ],
   "source": [
    "# number of rows in each dataset\n",
    "print(f\"Number of rows in df_all:    {len(df_all)}\")\n",
    "print(f\"Number of rows in df_bank_a: {len(df_bank_a)}\")\n",
    "print(f\"Number of rows in df_bank_b: {len(df_bank_b)}\")\n",
    "print(f\"Number of rows in df_bank_c: {len(df_bank_c)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test set that contains 20% of the data from each bank\n",
    "df_bank_a_test = df_bank_a.sample(frac=0.2, random_state=42)\n",
    "df_bank_b_test = df_bank_b.sample(frac=0.2, random_state=42)\n",
    "df_bank_c_test = df_bank_c.sample(frac=0.2, random_state=42)\n",
    "\n",
    "# Create a training set that contains the remaining 80% of the data from each bank\n",
    "df_bank_a_train = df_bank_a.drop(df_bank_a_test.index)\n",
    "df_bank_b_train = df_bank_b.drop(df_bank_b_test.index)\n",
    "df_bank_c_train = df_bank_c.drop(df_bank_c_test.index)\n",
    "\n",
    "# Create a validation set that contains 20% of the data from each bank\n",
    "df_bank_a_val = df_bank_a_train.sample(frac=0.2, random_state=42)\n",
    "df_bank_b_val = df_bank_b_train.sample(frac=0.2, random_state=42)\n",
    "df_bank_c_val = df_bank_c_train.sample(frac=0.2, random_state=42)\n",
    "\n",
    "# Create a training set that contains the remaining 80% of the data from each bank\n",
    "df_bank_a_train = df_bank_a_train.drop(df_bank_a_val.index)\n",
    "df_bank_b_train = df_bank_b_train.drop(df_bank_b_val.index)\n",
    "df_bank_c_train = df_bank_c_train.drop(df_bank_c_val.index)\n",
    "\n",
    "# Combine the training sets into one training set\n",
    "df_train = pd.concat([df_bank_a_train, df_bank_b_train, df_bank_c_train])\n",
    "\n",
    "# Combine the test sets into one test set\n",
    "df_test = pd.concat([df_bank_a_test, df_bank_b_test, df_bank_c_test])\n",
    "\n",
    "# Combine the validation sets into one validation set\n",
    "df_val = pd.concat([df_bank_a_val, df_bank_b_val, df_bank_c_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training and test sets into X and y\n",
    "X_train = df_train.drop('income', axis=1)\n",
    "y_train = df_train['income']\n",
    "X_test = df_test.drop('income', axis=1)\n",
    "y_test = df_test['income']\n",
    "X_val = df_val.drop('income', axis=1)\n",
    "y_val = df_val['income']\n",
    "\n",
    "X_train_bank_a = df_bank_a_train.drop('income', axis=1)\n",
    "y_train_bank_a = df_bank_a_train['income']\n",
    "X_test_bank_a = df_bank_a_test.drop('income', axis=1)\n",
    "y_test_bank_a = df_bank_a_test['income']\n",
    "\n",
    "X_train_bank_b = df_bank_b_train.drop('income', axis=1)\n",
    "y_train_bank_b = df_bank_b_train['income']\n",
    "X_test_bank_b = df_bank_b_test.drop('income', axis=1)\n",
    "y_test_bank_b = df_bank_b_test['income']\n",
    "\n",
    "X_train_bank_c = df_bank_c_train.drop('income', axis=1)\n",
    "y_train_bank_c = df_bank_c_train['income']\n",
    "X_test_bank_c = df_bank_c_test.drop('income', axis=1)\n",
    "y_test_bank_c = df_bank_c_test['income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train).astype(np.float32)\n",
    "y_train = np.asarray(y_train).astype(np.float32)\n",
    "\n",
    "X_test = np.asarray(X_test).astype(np.float32)\n",
    "y_test = np.asarray(y_test).astype(np.float32)\n",
    "\n",
    "X_val = np.asarray(X_val).astype(np.float32)\n",
    "y_val = np.asarray(y_val).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "        keras.layers.Dense(128, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "        keras.layers.Dropout(0.1),\n",
    "        keras.layers.Dense(40, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.1),\n",
    "        keras.layers.Dense(40, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.1),\n",
    "        keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 6s 5ms/step - loss: nan - accuracy: 0.7484 - val_loss: nan - val_accuracy: 0.7522\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.7513 - val_loss: nan - val_accuracy: 0.7522\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.7513 - val_loss: nan - val_accuracy: 0.7522\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.7513 - val_loss: nan - val_accuracy: 0.7522\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.7513 - val_loss: nan - val_accuracy: 0.7522\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.7513 - val_loss: nan - val_accuracy: 0.7522\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.7513 - val_loss: nan - val_accuracy: 0.7522\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.7513 - val_loss: nan - val_accuracy: 0.7522\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.7513 - val_loss: nan - val_accuracy: 0.7522\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.7513 - val_loss: nan - val_accuracy: 0.7522\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FML Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 3 # Bank A, B, C\n",
    "NUM_FML_ROUNDS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \"\"\"Constructs a simple model architecture suitable for the Dataset.\"\"\"\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Dense(128, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "        keras.layers.Dropout(0.1),\n",
    "        keras.layers.Dense(40, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.1),\n",
    "        keras.layers.Dense(40, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.1),\n",
    "        keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    tf.random.set_seed(seed_value)\n",
    "    model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialClient(fl.client.NumPyClient):\n",
    "    def __init__(self, trainset, valset) -> None:\n",
    "        # Create model\n",
    "        self.model = get_model()\n",
    "        self.trainset = trainset\n",
    "        self.valset = valset\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        tf.random.set_seed(seed_value)\n",
    "        self.model.fit(self.trainset[0], self.trainset[1], epochs=1, batch_size=512, verbose=VERBOSE)\n",
    "        return self.model.get_weights(), len(self.trainset), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        loss, acc = self.model.evaluate(self.valset[0], self.valset[1], verbose=VERBOSE)\n",
    "        return loss, len(self.valset), {\"accuracy\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_fn(global_train_datasets_list):\n",
    "    \"\"\"Return a function to construct a client.\n",
    "\n",
    "    The VirtualClientEngine will execute this function whenever a client is sampled by\n",
    "    the strategy to participate.\n",
    "    \"\"\"\n",
    "\n",
    "    def client_fn(cid: str) -> fl.client.Client:\n",
    "        \"\"\"Construct a DiabetesClient with its own dataset partition.\"\"\"\n",
    "\n",
    "        # Extract partition for client with id = cid\n",
    "        X, y = global_train_datasets_list[int(cid)]\n",
    "\n",
    "        # Now let's split it into train (90%) and validation (10%)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=seed_value)\n",
    "\n",
    "        trainset = (X_train, y_train)\n",
    "        valset = (X_val, y_val)\n",
    "\n",
    "        # Create and return client\n",
    "        return FinancialClient(trainset, valset)\n",
    "\n",
    "    return client_fn\n",
    "\n",
    "\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
    "    \"\"\"Aggregation function for (federated) evaluation metrics, i.e. those returned by\n",
    "    the client's evaluate() method.\"\"\"\n",
    "    # Multiply accuracy of each client by number of examples used\n",
    "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "\n",
    "    # Aggregate and return custom metric (weighted average)\n",
    "    return {\"accuracy\": sum(accuracies) / sum(examples)}\n",
    "\n",
    "\n",
    "def get_evaluate_fn(testset):\n",
    "    \"\"\"Return an evaluation function for server-side (i.e. centralised) evaluation.\"\"\"\n",
    "\n",
    "    # The `evaluate` function will be called after every round by the strategy\n",
    "    def evaluate(\n",
    "        server_round: int,\n",
    "        parameters: fl.common.NDArrays,\n",
    "        config: Dict[str, fl.common.Scalar],\n",
    "    ):\n",
    "        model = get_model()  # Construct the model\n",
    "        model.set_weights(parameters)  # Update model with the latest parameters\n",
    "        loss, accuracy = model.evaluate(testset[0], testset[1], verbose=VERBOSE)\n",
    "        return loss, {\"accuracy\": accuracy}\n",
    "\n",
    "    return evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comine train and validation sets for each bank\n",
    "df_bank_a_train_val = pd.concat([df_bank_a_train, df_bank_a_val])\n",
    "df_bank_b_train_val = pd.concat([df_bank_b_train, df_bank_b_val])\n",
    "df_bank_c_train_val = pd.concat([df_bank_c_train, df_bank_c_val])\n",
    "\n",
    "# Split data into X and y\n",
    "X_train_val_bank_a = df_bank_a_train_val.drop('income', axis=1)\n",
    "y_train_val_bank_a = df_bank_a_train_val['income']\n",
    "\n",
    "X_train_val_bank_b = df_bank_b_train_val.drop('income', axis=1)\n",
    "y_train_val_bank_b = df_bank_b_train_val['income']\n",
    "\n",
    "X_train_val_bank_c = df_bank_c_train_val.drop('income', axis=1)\n",
    "y_train_val_bank_c = df_bank_c_train_val['income']\n",
    "\n",
    "X_train_val_bank_a = np.asarray(X_train_val_bank_a).astype(np.float32)\n",
    "y_train_val_bank_a = np.asarray(y_train_val_bank_a).astype(np.float32)\n",
    "\n",
    "X_train_val_bank_b = np.asarray(X_train_val_bank_b).astype(np.float32)\n",
    "y_train_val_bank_b = np.asarray(y_train_val_bank_b).astype(np.float32)\n",
    "\n",
    "X_train_val_bank_c = np.asarray(X_train_val_bank_c).astype(np.float32)\n",
    "y_train_val_bank_c = np.asarray(y_train_val_bank_c).astype(np.float32)\n",
    "\n",
    "# Create a list of datasets for each bank\n",
    "global_train_datasets_list = [\n",
    "    (X_train_val_bank_a, y_train_val_bank_a),\n",
    "    (X_train_val_bank_b, y_train_val_bank_b),\n",
    "    (X_train_val_bank_c, y_train_val_bank_c),\n",
    "]\n",
    "\n",
    "# Create a test set\n",
    "df_test = pd.concat([df_bank_a_test, df_bank_b_test, df_bank_c_test])\n",
    "\n",
    "# Split data into X and y\n",
    "X_test = df_test.drop('income', axis=1)\n",
    "y_test = df_test['income']\n",
    "\n",
    "X_test = np.asarray(X_test).astype(np.float32)\n",
    "y_test = np.asarray(y_test).astype(np.float32)\n",
    "\n",
    "global_test_dataset = (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FedAvg strategy, considering all clients for training and evaluation\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=1,\n",
    "    fraction_evaluate=1,\n",
    "    min_fit_clients=NUM_CLIENTS,\n",
    "    min_evaluate_clients=NUM_CLIENTS,  \n",
    "    min_available_clients=NUM_CLIENTS, \n",
    "    evaluate_metrics_aggregation_fn=weighted_average,\n",
    "    evaluate_fn=get_evaluate_fn(global_test_dataset),\n",
    ")\n",
    "\n",
    "# Start simulation\n",
    "history_nn = fl.simulation.start_simulation(\n",
    "    client_fn=get_client_fn(global_train_datasets_list),\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=NUM_FML_ROUNDS),\n",
    "    strategy=strategy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.753237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.807875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.811506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.812881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.812725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.812775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.812981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.812550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.813625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.813637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.812612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.812331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.813531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.813450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.813263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.813025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.813450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.813112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.813044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.813281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    round  accuracy\n",
       "0       0  0.753237\n",
       "1       1  0.807875\n",
       "2       2  0.811506\n",
       "3       3  0.812881\n",
       "4       4  0.812725\n",
       "5       5  0.812775\n",
       "6       6  0.812981\n",
       "7       7  0.812550\n",
       "8       8  0.812500\n",
       "9       9  0.813625\n",
       "10     10  0.813637\n",
       "11     11  0.812612\n",
       "12     12  0.812331\n",
       "13     13  0.813531\n",
       "14     14  0.813450\n",
       "15     15  0.813263\n",
       "16     16  0.813025\n",
       "17     17  0.813450\n",
       "18     18  0.813112\n",
       "19     19  0.813044\n",
       "20     20  0.813281"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_value_nn = pd.DataFrame(history_nn.metrics_centralized['accuracy']).rename(columns={0: 'round', 1: 'accuracy'})\n",
    "accuracy_value_nn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
