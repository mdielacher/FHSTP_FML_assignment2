{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, confusion_matrix\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import flwr as fl\n",
    "from flwr.common import Metrics\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for reproducibility\n",
    "seed_value = 42\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "tf.keras.utils.set_random_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_a = pd.read_csv('../data/BankA.csv')\n",
    "df_bank_b = pd.read_csv('../data/BankB.csv')\n",
    "df_bank_c = pd.read_csv('../data/BankC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_bank_a, df_bank_b, df_bank_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip all string values from the dataset\n",
    "df = df_all.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "workclass\n",
       "Private             546342\n",
       "Self-emp-not-inc     66145\n",
       "Local-gov            51137\n",
       "unknown              47431\n",
       "State-gov            34717\n",
       "Self-emp-inc         27715\n",
       "Federal-gov          25879\n",
       "Not-working            633\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine Never-worked and Without-pay into one category\n",
    "df['workclass'] = df['workclass'].replace(['Never-worked', 'Without-pay'], 'Not-working')\n",
    "df['workclass'] = df['workclass'].replace(['?', '*'], 'unknown')\n",
    "df['workclass'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "marital-status\n",
       "Married                  368820\n",
       "Never-married            250510\n",
       "Divorced                 110459\n",
       "Widowed                   34203\n",
       "Separated                 25566\n",
       "Married-spouse-absent     10441\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine Married-civ-spouse and Married-AF-spouse into one category\n",
    "df['marital-status'] = df['marital-status'].replace(['Married-civ-spouse', 'Married-AF-spouse'], 'Married')\n",
    "df['marital-status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "occupation\n",
       "low        281223\n",
       "medium     259651\n",
       "high       211330\n",
       "unknown     47795\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace occupation by 4 categories (low, medium, high, unknown)\n",
    "df['occupation'] = df['occupation'].replace(['Exec-managerial', 'Prof-specialty'], 'high')\n",
    "df['occupation'] = df['occupation'].replace(['Armed-Forces', 'Protective-serv', 'Tech-support', 'Sales', 'Craft-repair', 'Transport-moving'], 'medium')\n",
    "df['occupation'] = df['occupation'].replace(['Adm-clerical', 'Machine-op-inspct', 'Farming-fishing', 'Handlers-cleaners', 'Other-service', 'Priv-house-serv'], 'low')\n",
    "df['occupation'] = df['occupation'].replace(['?', '*'], 'unknown')\n",
    "df['occupation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relationship\n",
       "Parent            362767\n",
       "Not-in-family     212898\n",
       "Own-child         119123\n",
       "Unmarried          80532\n",
       "Other-relative     24679\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine Husband and Wife into one category\n",
    "df['relationship'] = df['relationship'].replace(['Husband', 'Wife'], 'Parent')\n",
    "df['relationship'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "native-country\n",
       "North-America    753935\n",
       "Asia              14694\n",
       "Unknown           14390\n",
       "Europe            14273\n",
       "South-America      2707\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map native-country to continents\n",
    "df['native-country'] = df['native-country'].str.strip()\n",
    "df['native-country'] = df['native-country'].replace(['United-States', 'Puerto-Rico', 'Canada', 'Outlying-US(Guam-USVI-etc)', 'Cuba', 'Jamaica', 'Mexico', 'Dominican-Republic', 'El-Salvador', 'Guatemala', 'Haiti', 'Honduras', 'Nicaragua', 'Trinadad&Tobago', 'Peru', 'Ecuador', 'Columbia', 'Honduras', 'Haiti', 'Guatemala', 'El-Salvador', 'Dominican-Republic', 'Columbia', 'Ecuador', 'Peru', 'Jamaica', 'Mexico', 'Puerto-Rico', 'Cuba', 'Outlying-US(Guam-USVI-etc)', 'Canada', 'United-States'], 'North-America')\n",
    "df['native-country'] = df['native-country'].replace(['Germany', 'England', 'Italy', 'Poland', 'Portugal', 'Ireland', 'France', 'Yugoslavia', 'Scotland', 'Greece', 'Hungary', 'Holand-Netherlands'], 'Europe')\n",
    "df['native-country'] = df['native-country'].replace(['Philippines', 'India', 'China', 'Japan', 'Vietnam', 'Taiwan', 'Iran', 'Thailand', 'Hong', 'Cambodia', 'Laos'], 'Asia')\n",
    "df['native-country'] = df['native-country'].replace(['South', 'Columbia', 'Ecuador', 'Peru'], 'South-America')\n",
    "df['native-country'] = df['native-country'].replace(['Trinadad&Tobago', 'Honduras', 'Haiti', 'Guatemala', 'El-Salvador', 'Dominican-Republic', 'Columbia', 'Ecuador', 'Peru'], 'Central-America')\n",
    "df['native-country'] = df['native-country'].replace(['?', '*'], 'Unknown')\n",
    "df['native-country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "education\n",
       "HS-grad      258661\n",
       "higher       241834\n",
       "Bachelors    133796\n",
       "school       110209\n",
       "Masters       45697\n",
       "Doctorate      9802\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['education'] = df['education'].replace(['Preschool', '1st-4th', '5th-6th', '7th-8th', '9th', '10th', '11th', '12th'], 'school')\n",
    "df['education'] = df['education'].replace(['Assoc-voc', 'Assoc-acdm', 'Prof-school', 'Some-college'], 'higher')\n",
    "df['education'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "race\n",
       "White    686196\n",
       "Other    113803\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['race'] = df['race'].replace(['Black', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'Other'], 'Other')\n",
    "df['race'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age\n",
       "20-30    199748\n",
       "30-40    187599\n",
       "40-50    168168\n",
       "50-60    107704\n",
       "0-20      64609\n",
       "60-70     51995\n",
       "70-80     15668\n",
       "80-90      4508\n",
       "90+           0\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age'] = pd.cut(df['age'], bins=[0, 20, 30, 40, 50, 60, 70, 80, 90, 100], labels=['0-20', '20-30', '30-40', '40-50', '50-60', '60-70', '70-80', '80-90', '90+'])\n",
    "df['age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the fnlwgt column\n",
    "df.drop(['fnlwgt'], axis=1, inplace=True)\n",
    "\n",
    "# Drop the education column\n",
    "# df.drop(['education'], axis=1, inplace=True)\n",
    "\n",
    "# Drop the capital-gain column\n",
    "# df.drop(['capital-gain'], axis=1, inplace=True)\n",
    "\n",
    "# Drop the capital-loss column\n",
    "# df.drop(['capital-loss'], axis=1, inplace=True)\n",
    "\n",
    "# Drop the gender column\n",
    "df.drop(['gender'], axis=1, inplace=True)\n",
    "\n",
    "# Drop the age column\n",
    "df.drop(['age'], axis=1, inplace=True)\n",
    "\n",
    "# Drop the race column\n",
    "df.drop('race', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "income\n",
       "0    601449\n",
       "1    198550\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace income by 0 and 1\n",
    "df['income'] = df['income'].map({'<=50K': 0, '>50K': 1})\n",
    "df['income'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: institute\n",
      "institute\n",
      "Bank B    403240\n",
      "Bank A    226164\n",
      "Bank C    170595\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n",
      "Column: workclass\n",
      "workclass\n",
      "Private             546342\n",
      "Self-emp-not-inc     66145\n",
      "Local-gov            51137\n",
      "unknown              47431\n",
      "State-gov            34717\n",
      "Self-emp-inc         27715\n",
      "Federal-gov          25879\n",
      "Not-working            633\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n",
      "Column: education\n",
      "education\n",
      "HS-grad      258661\n",
      "higher       241834\n",
      "Bachelors    133796\n",
      "school       110209\n",
      "Masters       45697\n",
      "Doctorate      9802\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n",
      "Column: marital-status\n",
      "marital-status\n",
      "Married                  368820\n",
      "Never-married            250510\n",
      "Divorced                 110459\n",
      "Widowed                   34203\n",
      "Separated                 25566\n",
      "Married-spouse-absent     10441\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n",
      "Column: occupation\n",
      "occupation\n",
      "low        281223\n",
      "medium     259651\n",
      "high       211330\n",
      "unknown     47795\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n",
      "Column: relationship\n",
      "relationship\n",
      "Parent            362767\n",
      "Not-in-family     212898\n",
      "Own-child         119123\n",
      "Unmarried          80532\n",
      "Other-relative     24679\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n",
      "Column: native-country\n",
      "native-country\n",
      "North-America    753935\n",
      "Asia              14694\n",
      "Unknown           14390\n",
      "Europe            14273\n",
      "South-America      2707\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n",
      "Column: educational-num\n",
      "(8.5, 11.0]     462456\n",
      "(11.0, 13.5]    158326\n",
      "(13.5, 16.0]     69009\n",
      "(3.5, 6.0]       53811\n",
      "(6.0, 8.5]       41887\n",
      "(0.984, 3.5]     14510\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n",
      "Column: capital-gain\n",
      "(-100.0, 16666.5]     792260\n",
      "(83332.5, 99999.0]      4665\n",
      "(16666.5, 33333.0]      3019\n",
      "(33333.0, 49999.5]        55\n",
      "(49999.5, 66666.0]         0\n",
      "(66666.0, 83332.5]         0\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n",
      "Column: capital-loss\n",
      "(-3.6839999999999997, 613.833]    760671\n",
      "(1841.5, 2455.333]                 25034\n",
      "(1227.667, 1841.5]                 12776\n",
      "(2455.333, 3069.167]                 788\n",
      "(613.833, 1227.667]                  540\n",
      "(3069.167, 3683.0]                   190\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n",
      "Column: hours-per-week\n",
      "(33.667, 50.0]      555199\n",
      "(17.333, 33.667]    105827\n",
      "(50.0, 66.333]       72075\n",
      "(0.901, 17.333]      44806\n",
      "(66.333, 82.667]     16324\n",
      "(82.667, 99.0]        5768\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n",
      "Column: income\n",
      "(-0.002, 0.167]    601449\n",
      "(0.833, 1.0]       198550\n",
      "(0.167, 0.333]          0\n",
      "(0.333, 0.5]            0\n",
      "(0.5, 0.667]            0\n",
      "(0.667, 0.833]          0\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    " \n",
    "for column in df[categorical_columns].columns:\n",
    "    print(f\"Column: {column}\")\n",
    "    print(df[column].value_counts())\n",
    "    print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# Print also the numberical columns, categorize them into bins of 6\n",
    "numerical_columns = df.select_dtypes(include=['int64']).columns\n",
    "\n",
    "for column in df[numerical_columns].columns:\n",
    "    print(f\"Column: {column}\")\n",
    "    print(df[column].value_counts(bins=6))\n",
    "    print(\"\\n\" + \"=\"*30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the categorical columns\n",
    "df = pd.get_dummies(df, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop institutes columns\n",
    "df_all = df.drop(['institute_Bank A', 'institute_Bank B', 'institute_Bank C'], axis=1)\n",
    "df_bank_a = df[df['institute_Bank A'] == 1].drop(['institute_Bank A', 'institute_Bank B', 'institute_Bank C'], axis=1)\n",
    "df_bank_b = df[df['institute_Bank B'] == 1].drop(['institute_Bank A', 'institute_Bank B', 'institute_Bank C'], axis=1)\n",
    "df_bank_c = df[df['institute_Bank C'] == 1].drop(['institute_Bank A', 'institute_Bank B', 'institute_Bank C'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in df_all:    799999\n",
      "Number of rows in df_bank_a: 226164\n",
      "Number of rows in df_bank_b: 403240\n",
      "Number of rows in df_bank_c: 170595\n"
     ]
    }
   ],
   "source": [
    "# number of rows in each dataset\n",
    "print(f\"Number of rows in df_all:    {len(df_all)}\")\n",
    "print(f\"Number of rows in df_bank_a: {len(df_bank_a)}\")\n",
    "print(f\"Number of rows in df_bank_b: {len(df_bank_b)}\")\n",
    "print(f\"Number of rows in df_bank_c: {len(df_bank_c)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test set that contains 20% of the data from each bank\n",
    "df_bank_a_test = df_bank_a.sample(frac=0.2, random_state=42)\n",
    "df_bank_b_test = df_bank_b.sample(frac=0.2, random_state=42)\n",
    "df_bank_c_test = df_bank_c.sample(frac=0.2, random_state=42)\n",
    "\n",
    "# Create a training set that contains the remaining 80% of the data from each bank\n",
    "df_bank_a_train = df_bank_a.drop(df_bank_a_test.index)\n",
    "df_bank_b_train = df_bank_b.drop(df_bank_b_test.index)\n",
    "df_bank_c_train = df_bank_c.drop(df_bank_c_test.index)\n",
    "\n",
    "# Create a validation set that contains 20% of the data from each bank\n",
    "df_bank_a_val = df_bank_a_train.sample(frac=0.2, random_state=42)\n",
    "df_bank_b_val = df_bank_b_train.sample(frac=0.2, random_state=42)\n",
    "df_bank_c_val = df_bank_c_train.sample(frac=0.2, random_state=42)\n",
    "\n",
    "# Create a training set that contains the remaining 80% of the data from each bank\n",
    "df_bank_a_train = df_bank_a_train.drop(df_bank_a_val.index)\n",
    "df_bank_b_train = df_bank_b_train.drop(df_bank_b_val.index)\n",
    "df_bank_c_train = df_bank_c_train.drop(df_bank_c_val.index)\n",
    "\n",
    "# Combine the training sets into one training set\n",
    "df_train = pd.concat([df_bank_a_train, df_bank_b_train, df_bank_c_train])\n",
    "\n",
    "# Combine the test sets into one test set\n",
    "df_test = pd.concat([df_bank_a_test, df_bank_b_test, df_bank_c_test])\n",
    "\n",
    "# Combine the validation sets into one validation set\n",
    "df_val = pd.concat([df_bank_a_val, df_bank_b_val, df_bank_c_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into X and y\n",
    "X_train = df_train.drop('income', axis=1)\n",
    "y_train = df_train['income']\n",
    "\n",
    "X_test = df_test.drop('income', axis=1)\n",
    "y_test = df_test['income']\n",
    "\n",
    "X_val = df_val.drop('income', axis=1)\n",
    "y_val = df_val['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train).astype(np.float32)\n",
    "y_train = np.asarray(y_train).astype(np.float32)\n",
    "\n",
    "X_test = np.asarray(X_test).astype(np.float32)\n",
    "y_test = np.asarray(y_test).astype(np.float32)\n",
    "\n",
    "X_val = np.asarray(X_val).astype(np.float32)\n",
    "y_val = np.asarray(y_val).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(128, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(16, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4000/4000 [==============================] - 12s 3ms/step - loss: 2.7668 - accuracy: 0.7507 - val_loss: 0.5600 - val_accuracy: 0.7522\n",
      "Epoch 2/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.5607 - accuracy: 0.7513 - val_loss: 0.5599 - val_accuracy: 0.7522\n",
      "Epoch 3/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.5557 - accuracy: 0.7513 - val_loss: 0.5408 - val_accuracy: 0.7522\n",
      "Epoch 4/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.5050 - accuracy: 0.7513 - val_loss: 0.4326 - val_accuracy: 0.7522\n",
      "Epoch 5/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.4523 - accuracy: 0.7512 - val_loss: 0.4103 - val_accuracy: 0.7522\n",
      "Epoch 6/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.4403 - accuracy: 0.7513 - val_loss: 0.4012 - val_accuracy: 0.7522\n",
      "Epoch 7/50\n",
      "4000/4000 [==============================] - 12s 3ms/step - loss: 0.4256 - accuracy: 0.7735 - val_loss: 0.4003 - val_accuracy: 0.8017\n",
      "Epoch 8/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.4166 - accuracy: 0.7882 - val_loss: 0.4003 - val_accuracy: 0.8065\n",
      "Epoch 9/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.4130 - accuracy: 0.7909 - val_loss: 0.3923 - val_accuracy: 0.7976\n",
      "Epoch 10/50\n",
      "4000/4000 [==============================] - 12s 3ms/step - loss: 0.4101 - accuracy: 0.7922 - val_loss: 0.3857 - val_accuracy: 0.8089\n",
      "Epoch 11/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.4083 - accuracy: 0.7927 - val_loss: 0.3857 - val_accuracy: 0.8092\n",
      "Epoch 12/50\n",
      "4000/4000 [==============================] - 12s 3ms/step - loss: 0.4060 - accuracy: 0.7951 - val_loss: 0.4051 - val_accuracy: 0.8018\n",
      "Epoch 13/50\n",
      "4000/4000 [==============================] - 12s 3ms/step - loss: 0.4037 - accuracy: 0.7981 - val_loss: 0.3882 - val_accuracy: 0.8081\n",
      "Epoch 14/50\n",
      "4000/4000 [==============================] - 12s 3ms/step - loss: 0.4008 - accuracy: 0.7993 - val_loss: 0.3842 - val_accuracy: 0.8096\n",
      "Epoch 15/50\n",
      "4000/4000 [==============================] - 12s 3ms/step - loss: 0.3987 - accuracy: 0.8007 - val_loss: 0.3882 - val_accuracy: 0.8089\n",
      "Epoch 16/50\n",
      "4000/4000 [==============================] - 15s 4ms/step - loss: 0.3977 - accuracy: 0.8018 - val_loss: 0.3883 - val_accuracy: 0.8062\n",
      "Epoch 17/50\n",
      "4000/4000 [==============================] - 21s 5ms/step - loss: 0.3969 - accuracy: 0.8020 - val_loss: 0.3830 - val_accuracy: 0.8098\n",
      "Epoch 18/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.3974 - accuracy: 0.8000 - val_loss: 0.3857 - val_accuracy: 0.8090\n",
      "Epoch 19/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.3966 - accuracy: 0.8061 - val_loss: 0.3872 - val_accuracy: 0.8043\n",
      "Epoch 20/50\n",
      "4000/4000 [==============================] - 12s 3ms/step - loss: 0.3959 - accuracy: 0.8059 - val_loss: 0.3884 - val_accuracy: 0.8085\n",
      "Epoch 21/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.3947 - accuracy: 0.8069 - val_loss: 0.3888 - val_accuracy: 0.8092\n",
      "Epoch 22/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.3949 - accuracy: 0.8066 - val_loss: 0.3842 - val_accuracy: 0.8083\n",
      "Epoch 23/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.3941 - accuracy: 0.8069 - val_loss: 0.3838 - val_accuracy: 0.8076\n",
      "Epoch 24/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.3938 - accuracy: 0.8069 - val_loss: 0.3836 - val_accuracy: 0.8100\n",
      "Epoch 25/50\n",
      "4000/4000 [==============================] - 13s 3ms/step - loss: 0.3935 - accuracy: 0.8075 - val_loss: 0.3822 - val_accuracy: 0.8093\n",
      "Epoch 26/50\n",
      "4000/4000 [==============================] - 14s 3ms/step - loss: 0.3930 - accuracy: 0.8076 - val_loss: 0.3913 - val_accuracy: 0.8081\n",
      "Epoch 27/50\n",
      "4000/4000 [==============================] - 21s 5ms/step - loss: 0.3930 - accuracy: 0.8077 - val_loss: 0.3852 - val_accuracy: 0.8088\n",
      "Epoch 28/50\n",
      "4000/4000 [==============================] - 13s 3ms/step - loss: 0.3927 - accuracy: 0.8077 - val_loss: 0.3825 - val_accuracy: 0.8100\n",
      "Epoch 29/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.3924 - accuracy: 0.8075 - val_loss: 0.3852 - val_accuracy: 0.8059\n",
      "Epoch 30/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.3923 - accuracy: 0.8076 - val_loss: 0.3819 - val_accuracy: 0.8096\n",
      "Epoch 31/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.3924 - accuracy: 0.8078 - val_loss: 0.3833 - val_accuracy: 0.8092\n",
      "Epoch 32/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.3919 - accuracy: 0.8082 - val_loss: 0.3875 - val_accuracy: 0.8032\n",
      "Epoch 33/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.3916 - accuracy: 0.8079 - val_loss: 0.3846 - val_accuracy: 0.8086\n",
      "Epoch 34/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.3911 - accuracy: 0.8081 - val_loss: 0.3944 - val_accuracy: 0.8040\n",
      "Epoch 35/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.3910 - accuracy: 0.8082 - val_loss: 0.3832 - val_accuracy: 0.8100\n",
      "Epoch 36/50\n",
      "4000/4000 [==============================] - 22s 6ms/step - loss: 0.3909 - accuracy: 0.8083 - val_loss: 0.3816 - val_accuracy: 0.8100\n",
      "Epoch 37/50\n",
      "4000/4000 [==============================] - 16s 4ms/step - loss: 0.3908 - accuracy: 0.8082 - val_loss: 0.3841 - val_accuracy: 0.8089\n",
      "Epoch 38/50\n",
      "4000/4000 [==============================] - 13s 3ms/step - loss: 0.3903 - accuracy: 0.8085 - val_loss: 0.3816 - val_accuracy: 0.8096\n",
      "Epoch 39/50\n",
      "4000/4000 [==============================] - 14s 3ms/step - loss: 0.3905 - accuracy: 0.8084 - val_loss: 0.3831 - val_accuracy: 0.8092\n",
      "Epoch 40/50\n",
      "4000/4000 [==============================] - 15s 4ms/step - loss: 0.3906 - accuracy: 0.8080 - val_loss: 0.3826 - val_accuracy: 0.8097\n",
      "Epoch 41/50\n",
      "4000/4000 [==============================] - 12s 3ms/step - loss: 0.3901 - accuracy: 0.8085 - val_loss: 0.3836 - val_accuracy: 0.8077\n",
      "Epoch 42/50\n",
      "4000/4000 [==============================] - 12s 3ms/step - loss: 0.3903 - accuracy: 0.8083 - val_loss: 0.3823 - val_accuracy: 0.8098\n",
      "Epoch 43/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.3900 - accuracy: 0.8084 - val_loss: 0.3836 - val_accuracy: 0.8085\n",
      "Epoch 44/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.3900 - accuracy: 0.8082 - val_loss: 0.3857 - val_accuracy: 0.8061\n",
      "Epoch 45/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.3900 - accuracy: 0.8087 - val_loss: 0.3828 - val_accuracy: 0.8094\n",
      "Epoch 46/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.3897 - accuracy: 0.8084 - val_loss: 0.3829 - val_accuracy: 0.8096\n",
      "Epoch 47/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.3901 - accuracy: 0.8085 - val_loss: 0.3903 - val_accuracy: 0.8090\n",
      "Epoch 48/50\n",
      "4000/4000 [==============================] - 12s 3ms/step - loss: 0.3896 - accuracy: 0.8090 - val_loss: 0.3863 - val_accuracy: 0.8062\n",
      "Epoch 49/50\n",
      "4000/4000 [==============================] - 12s 3ms/step - loss: 0.3897 - accuracy: 0.8085 - val_loss: 0.3874 - val_accuracy: 0.8038\n",
      "Epoch 50/50\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.3895 - accuracy: 0.8089 - val_loss: 0.3842 - val_accuracy: 0.8093\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 8s 2ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[104068  16451]\n",
      " [ 14119  25362]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8089375"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FML Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "VERBOSE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 3 # Bank A, B, C\n",
    "NUM_FML_ROUNDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \"\"\"Constructs a simple model architecture suitable for the Dataset.\"\"\"\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Dense(128, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(16, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.1),\n",
    "        keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    tf.random.set_seed(seed_value)\n",
    "    model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialClient(fl.client.NumPyClient):\n",
    "    def __init__(self, trainset, valset) -> None:\n",
    "        # Create model\n",
    "        self.model = get_model()\n",
    "        self.trainset = trainset\n",
    "        self.valset = valset\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        tf.random.set_seed(seed_value)\n",
    "        self.model.fit(self.trainset[0], self.trainset[1], epochs=5, verbose=VERBOSE)\n",
    "        return self.model.get_weights(), len(self.trainset), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        loss, acc = self.model.evaluate(self.valset[0], self.valset[1], verbose=VERBOSE)\n",
    "        return loss, len(self.valset), {\"accuracy\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_fn(global_train_datasets_list):\n",
    "    \"\"\"Return a function to construct a client.\n",
    "\n",
    "    The VirtualClientEngine will execute this function whenever a client is sampled by\n",
    "    the strategy to participate.\n",
    "    \"\"\"\n",
    "\n",
    "    def client_fn(cid: str) -> fl.client.Client:\n",
    "        \"\"\"Construct a DiabetesClient with its own dataset partition.\"\"\"\n",
    "\n",
    "        # Extract partition for client with id = cid\n",
    "        X, y = global_train_datasets_list[int(cid)]\n",
    "\n",
    "        # Now let's split it into train (90%) and validation (10%)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=seed_value)\n",
    "\n",
    "        trainset = (X_train, y_train)\n",
    "        valset = (X_val, y_val)\n",
    "\n",
    "        # Create and return client\n",
    "        return FinancialClient(trainset, valset)\n",
    "\n",
    "    return client_fn\n",
    "\n",
    "\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
    "    \"\"\"Aggregation function for (federated) evaluation metrics, i.e. those returned by\n",
    "    the client's evaluate() method.\"\"\"\n",
    "    # Multiply accuracy of each client by number of examples used\n",
    "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "\n",
    "    # Aggregate and return custom metric (weighted average)\n",
    "    return {\"accuracy\": sum(accuracies) / sum(examples)}\n",
    "\n",
    "\n",
    "def get_evaluate_fn(testset):\n",
    "    \"\"\"Return an evaluation function for server-side (i.e. centralised) evaluation.\"\"\"\n",
    "\n",
    "    # The `evaluate` function will be called after every round by the strategy\n",
    "    def evaluate(\n",
    "        server_round: int,\n",
    "        parameters: fl.common.NDArrays,\n",
    "        config: Dict[str, fl.common.Scalar],\n",
    "    ):\n",
    "        model = get_model()  # Construct the model\n",
    "        model.set_weights(parameters)  # Update model with the latest parameters\n",
    "        loss, accuracy = model.evaluate(testset[0], testset[1], verbose=VERBOSE)\n",
    "        return loss, {\"accuracy\": accuracy}\n",
    "\n",
    "    return evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comine train and validation sets for each bank\n",
    "df_bank_a_train_val = pd.concat([df_bank_a_train, df_bank_a_val])\n",
    "df_bank_b_train_val = pd.concat([df_bank_b_train, df_bank_b_val])\n",
    "df_bank_c_train_val = pd.concat([df_bank_c_train, df_bank_c_val])\n",
    "\n",
    "# Split data into X and y\n",
    "X_train_val_bank_a = df_bank_a_train_val.drop('income', axis=1)\n",
    "y_train_val_bank_a = df_bank_a_train_val['income']\n",
    "\n",
    "X_train_val_bank_b = df_bank_b_train_val.drop('income', axis=1)\n",
    "y_train_val_bank_b = df_bank_b_train_val['income']\n",
    "\n",
    "X_train_val_bank_c = df_bank_c_train_val.drop('income', axis=1)\n",
    "y_train_val_bank_c = df_bank_c_train_val['income']\n",
    "\n",
    "X_train_val_bank_a = np.asarray(X_train_val_bank_a).astype(np.float32)\n",
    "y_train_val_bank_a = np.asarray(y_train_val_bank_a).astype(np.float32)\n",
    "\n",
    "X_train_val_bank_b = np.asarray(X_train_val_bank_b).astype(np.float32)\n",
    "y_train_val_bank_b = np.asarray(y_train_val_bank_b).astype(np.float32)\n",
    "\n",
    "X_train_val_bank_c = np.asarray(X_train_val_bank_c).astype(np.float32)\n",
    "y_train_val_bank_c = np.asarray(y_train_val_bank_c).astype(np.float32)\n",
    "\n",
    "# Create a list of datasets for each bank\n",
    "global_train_datasets_list = [\n",
    "    (X_train_val_bank_a, y_train_val_bank_a),\n",
    "    (X_train_val_bank_b, y_train_val_bank_b),\n",
    "    (X_train_val_bank_c, y_train_val_bank_c),\n",
    "]\n",
    "\n",
    "# Create a test set\n",
    "df_test = pd.concat([df_bank_a_test, df_bank_b_test, df_bank_c_test])\n",
    "\n",
    "# Split data into X and y\n",
    "X_test = df_test.drop('income', axis=1)\n",
    "y_test = df_test['income']\n",
    "\n",
    "X_test = np.asarray(X_test).astype(np.float32)\n",
    "y_test = np.asarray(y_test).astype(np.float32)\n",
    "\n",
    "global_test_dataset = (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FedAvg strategy, considering all clients for training and evaluation\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=1,\n",
    "    fraction_evaluate=1,\n",
    "    min_fit_clients=NUM_CLIENTS,\n",
    "    min_evaluate_clients=NUM_CLIENTS,  \n",
    "    min_available_clients=NUM_CLIENTS, \n",
    "    evaluate_metrics_aggregation_fn=weighted_average,\n",
    "    evaluate_fn=get_evaluate_fn(global_test_dataset),\n",
    ")\n",
    "\n",
    "# Start simulation\n",
    "history_nn = fl.simulation.start_simulation(\n",
    "    client_fn=get_client_fn(global_train_datasets_list),\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=NUM_FML_ROUNDS),\n",
    "    strategy=strategy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.246756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.810069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.810387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.809656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.810094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.809869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   round  accuracy\n",
       "0      0  0.246756\n",
       "1      1  0.810069\n",
       "2      2  0.810387\n",
       "3      3  0.809656\n",
       "4      4  0.810094\n",
       "5      5  0.809869"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_value_nn = pd.DataFrame(history_nn.metrics_centralized['accuracy']).rename(columns={0: 'round', 1: 'accuracy'})\n",
    "accuracy_value_nn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
